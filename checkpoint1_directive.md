# CS145 Recommendation System Competition - Checkpoint 1: Content-Based Recommenders

## Overview

The first checkpoint focuses on **Content-Based Recommenders** that leverage user and item attributes to make effective recommendations. Your goal is to implement algorithms that utilize these attributes to maximize revenue by recommending the right items to the right users.

## Task

Implement at least three different content-based recommendation approaches using different methods for handling tabular features. Your implementation should:

1. Process user and item attribute data
2. Build models that predict user preferences based on these attributes
3. Generate ranked recommendations that maximize revenue
4. Compare the performance of different approaches

## Dataset

You'll work with the synthetic dataset generated by the Sim4Rec framework, which includes:

- **User attributes**: demographic information, preferences, and behavioral traits
- **Item attributes**: category, price, features, and other properties
- **Interaction data**: historical user-item interactions from training iterations

## Implementation Requirements

### 1. Feature Processing

- Extract relevant features from user and item attributes
- Implement feature normalization/scaling
- Consider feature engineering to create more informative representations
- Handle categorical features appropriately (one-hot encoding, embedding, etc.)

### 2. Model Implementation

Implement at least three different approaches from the following:

a) **K-Nearest Neighbors (kNN)**:
   - Experiment with different distance metrics (Euclidean, cosine, Manhattan)
   - Try different values of k (3, 5, 10, 20)
   - Consider user-based and item-based similarity approaches

b) **Logistic Regression**:
   - Implement binary classification to predict purchase likelihood
   - Try different regularization approaches (L1, L2, ElasticNet)
   - Tune regularization strength parameters

c) **Decision Trees**:
   - Implement decision tree classifiers
   - Tune tree depth, minimum samples per leaf, and other hyperparameters
   - Consider pruning techniques to prevent overfitting

d) **Random Forest**:
   - Implement ensemble of decision trees
   - Tune number of trees, maximum features, and other hyperparameters
   - Analyze feature importance

e) **Gradient Boosting**:
   - Implement boosted tree models (XGBoost, LightGBM, etc.)
   - Tune learning rate, number of estimators, and tree-specific parameters
   - Implement early stopping to prevent overfitting

### 3. Ranking Optimization

Implement effective ranking strategies:

- **Use logit outputs for ranking**: Sort recommendations by predicted probability
- **Incorporate price information**: Consider expected revenue (price Ã— probability)
- **Position bias**: Account for the effect of item position on click/purchase probability

### 4. Regularization Techniques

Experiment with at least two regularization schemes:

- **L1 regularization**: Encourages sparse feature selection
- **L2 regularization**: Prevents large coefficient values
- **Dropout**: For neural network approaches
- **Early stopping**: Prevent overfitting by monitoring validation performance
- **Cross-validation**: Ensure robust hyperparameter selection

### 5. Evaluation and Comparison

Compare your implemented models using:

- **Primary metric**: Total revenue
- **Secondary metrics**: Discounted revenue, Precision@K, NDCG@K, MRR, Hit Rate
- **Learning curves**: Performance across training iterations
- **Ablation studies**: Impact of different features and hyperparameters

## Optimization Tips

1. **Ranking optimization**: Use model outputs (logits/probabilities) to rank items
2. **Revenue-aware training**: 
   - Consider incorporating item prices into your objective function
   - Optimize for expected revenue rather than just click/purchase probability
3. **Regularization tuning**:
   - Start with a reasonable range of regularization parameters
   - Use cross-validation to find optimal regularization strength
   - Consider the bias-variance tradeoff in your model selection
4. **Feature importance analysis**:
   - Identify which user and item features contribute most to performance
   - Prune irrelevant features that may introduce noise

## Allowed Tools and Libraries

You may use the following data science packages:
- pandas
- numpy
- scikit-learn
- pytorch (if you want to implement neural approaches)
- matplotlib/seaborn (for visualization)
- Any sklearn-compatible libraries (XGBoost, LightGBM, etc.)

## Deliverables

Be prepared to submit the following as part of the final deliverables:

1. **Code**: Your implementation in a well-organized Python module
2. **Report** containing:
   - Description of implemented approaches
   - Feature engineering techniques used
   - Hyperparameter tuning methodology and results
   - Performance comparison between different models
   - Analysis of what worked well and what didn't

3. **Presentation slides** (5-8 slides) summarizing your approach and results

## Grading Criteria (Tentative) 

- **Implementation correctness**: 40%
- **Model performance**: 30%
- **Approach diversity**: 15%
- **Report quality and insights**: 15%

## Getting Started

1. Clone the repository and set up the environment as described in the main README
2. Familiarize yourself with the `recommender_analysis_visualization.py` script
3. Start by implementing a simple content-based model as a baseline
4. Gradually improve your approach by adding more sophisticated methods
5. Use the provided evaluation framework to compare different approaches

Remember: The goal is to maximize **discounted revenue** by recommending items that users are likely to purchase at their given prices! 